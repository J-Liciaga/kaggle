{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Classification Finetuning\n",
    "\n",
    "### Overview\n",
    "\n",
    "This notebook contains code for finetuning a pre-trained LLM for classification tasks. The goal is to train an LLM to classify whether a given response is better than the other response, based on the prompt.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "The dataset is a CSV file containing the following columns:\n",
    "\n",
    "- prompt: a prompt for the game\n",
    "- response_a: a response to the prompt\n",
    "- response_b: another response to the prompt\n",
    "- winner_model_a: whether response_a is better than response_b\n",
    "- winner_model_b: whether response_b is better than response_a\n",
    "- winner_tie: whether response_a and response_b are tied\n",
    "\n",
    "### Model\n",
    "\n",
    "The model used for finetuning is a pre-trained LLM from Hugging Face's model hub. The model used in this notebook is `bert-base-uncased`.\n",
    "\n",
    "### Training\n",
    "\n",
    "The training loop is implemented using PyTorch. The model is trained for 100 epochs with a batch size of 16. The optimizer used is AdamW. The loss function used is Cross-Entropy Loss.\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "The model is evaluated on the validation set. The validation loss is computed using the validation set.\n",
    "\n",
    "### Submission\n",
    "\n",
    "The submission file is generated using the test set. The submission file is a CSV file containing the following columns:\n",
    "\n",
    "- id: the id of the test data\n",
    "- winner_model_a: the probability that response_a is better than response_b\n",
    "- winner_model_b: the probability that response_b is better than response_a\n",
    "- winner_tie: the probability that response_a and response_b are tied\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (2.4.1)\n",
      "Requirement already satisfied: transformers in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (4.45.2)\n",
      "Requirement already satisfied: scikit-learn in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (1.5.2)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: seaborn in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (0.13.2)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from torch) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from torch) (4.12.1)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from torch) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from torch) (2024.6.0)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from transformers) (0.23.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from seaborn) (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from requests->transformers) (2024.6.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch transformers scikit-learn pandas numpy seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import BertTokenizer, BertModel, AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Base Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"./data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_name = \"bert-base-uncased\"\n",
    "    batch_size = 16\n",
    "    lr = 5e-5\n",
    "    epochs = 5\n",
    "    max_length = 256\n",
    "    num_classes= 3\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(CFG.model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "train_df = pd.read_csv(BASE_PATH + \"original/train.csv\")\n",
    "test_df = pd.read_csv(BASE_PATH + \"original/test.csv\")\n",
    "\n",
    "# tokenize training data\n",
    "def tokenize_data(df):\n",
    "    inputs = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        prompt = row[\"prompt\"]\n",
    "        response_a = row[\"response_a\"]\n",
    "        response_b = row[\"response_b\"]\n",
    "        \n",
    "        # concatenate prompt and response\n",
    "        input_str = f\"Prompt: {prompt} Response A: {response_a} Response B: {response_b}\"\n",
    "\n",
    "        encoded_input = tokenizer(\n",
    "            input_str, \n",
    "            truncation=True, \n",
    "            padding=\"max_length\",\n",
    "            max_length=CFG.max_length, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        inputs.append(encoded_input)\n",
    "\n",
    "    return inputs\n",
    "\n",
    "train_inputs = tokenize_data(train_df)\n",
    "\n",
    "# tokenize test data\n",
    "def tokenize_test_data(df):\n",
    "    inputs = []\n",
    "    ids = []\n",
    "    for _, row in df.iterrows():\n",
    "        prompt = row['prompt']\n",
    "        response_a = row['response_a']\n",
    "        response_b = row['response_b']\n",
    "\n",
    "        # concatenate prompt and response\n",
    "        input_str = f\"Prompt: {prompt} Response A: {response_a} Response B: {response_b}\"\n",
    "\n",
    "        encoded_input = tokenizer(\n",
    "            input_str,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=CFG.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        inputs.append(encoded_input)\n",
    "        \n",
    "        ids.append(row['id'])  # keep track of the IDs for the submission file\n",
    "\n",
    "    return inputs, ids\n",
    "\n",
    "test_inputs, test_ids = tokenize_test_data(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "This model takes tokenized inputs, runs it through the pretrained BERT model, and adds a classification layer on top to predict one of the three classes:\n",
    "- model_a wins\n",
    "- model_b wins\n",
    "- tie\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreferenceClassifier(nn.Module):\n",
    "    def __init__(self, model_name=CFG.model_name, num_classes = CFG.num_classes):\n",
    "        super(PreferenceClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output # CLS token representation\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Feed the tokenized inputs to the model and compute the loss using cross-entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreferenceDataset(Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.labels[idx]\n",
    "\n",
    "train_labels = train_df[\"winner_model_a\"].values\n",
    "train_dataset = PreferenceDataset(train_inputs, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True)\n",
    "\n",
    "# initialize model, optimizer, and loss fn\n",
    "model = PreferenceClassifier()\n",
    "model.to(CFG.device)\n",
    "optimizer = AdamW(model.parameters(), lr=CFG.lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loss = []  # store training loss after each epoch\n",
    "true_labels = []   # store all ground truth labels\n",
    "predicted_labels = []  # store all predicted labels\n",
    "predicted_probs = []  # store predicted probabilities for each class\n",
    "\n",
    "# training loop\n",
    "for epoch in range(CFG.epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        inputs, labels = batch\n",
    "        \n",
    "        input_ids = inputs[\"input_ids\"].squeeze(1).to(CFG.device)\n",
    "        attention_mask = inputs[\"attention_mask\"].squeeze(1).to(CFG.device)\n",
    "        labels = labels.to(CFG.device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # accumulate loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # store true and predicted labels\n",
    "        true_labels.extend(labels.cpu().numpy())  # Ground truth\n",
    "        predictions = torch.argmax(outputs, dim=1).cpu().numpy()  # Predicted class\n",
    "        predicted_labels.extend(predictions)\n",
    "\n",
    "        # store predicted probabilities (softmax output)\n",
    "        probs = torch.softmax(outputs, dim=1).detach().cpu().numpy()\n",
    "        predicted_probs.extend(probs)\n",
    "\n",
    "    # average loss over batches\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_loss.append(avg_train_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{CFG.epochs}, Loss: {avg_train_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom Dataset for test data\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, inputs, ids):\n",
    "        self.inputs = inputs\n",
    "        self.ids = ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.ids[idx]\n",
    "\n",
    "# create the test Dataset and DataLoader\n",
    "test_dataset = TestDataset(test_inputs, test_ids)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "evaluate using log loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs, labels = batch\n",
    "            input_ids = inputs[\"input_ids\"].squeeze(1).to(CFG.device)\n",
    "            attention_mask = inputs[\"attention_mask\"].squeeze(1).to(CFG.device)\n",
    "            labels = labels.to(CFG.device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            preds = torch.softmax(outputs, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    preds = np.concatenate(all_preds, axis=0)\n",
    "    labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    return log_loss(labels, preds)\n",
    "            \n",
    "# evaluate model on validation set\n",
    "val_loss = evaluate(model, train_loader)\n",
    "print(f\"Validation Loss: {val_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of Labels\n",
    "\n",
    "whether the classes are balanced or if any imbalance needs to be addressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "train_df['winner'].value_counts().plot(kind='bar')\n",
    "plt.title(\"Distribution of Outcomes (Model A, Model B, Tie)\")\n",
    "plt.xlabel(\"Outcome\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of Response Lengths\n",
    "\n",
    "analyze how the lengths of responses from each LLM varies, as this can impact user preference. Useful because long or short responses might correleate with preference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['response_a_length'] = train_df['response_a'].apply(len)\n",
    "train_df['response_b_length'] = train_df['response_b'].apply(len)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(train_df['response_a_length'], bins=50, alpha=0.6, label='Response A Lengths')\n",
    "plt.hist(train_df['response_b_length'], bins=50, alpha=0.6, label='Response B Lengths')\n",
    "plt.title(\"Distribution of Response Lengths (Model A vs. Model B)\")\n",
    "plt.xlabel(\"Response Length\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Similarity\n",
    "\n",
    "visualize the similarity between responses from each LLM. It could help determine whether the responses are generally similar or different (which may influence user preference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate TF-IDF-based similarity between responses\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(train_df['response_a'] + train_df['response_b'])\n",
    "cosine_similarities = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# plot histogram of similarity scores\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(cosine_similarities.diagonal(), bins=50, alpha=0.7)\n",
    "plt.title(\"Distribution of Cosine Similarities Between Responses\")\n",
    "plt.xlabel(\"Cosine Similarity\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance and Diagnostics - Loss Curve\n",
    "\n",
    "visualize model overfitting, underfitting, or training as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.title(\"Training and Validation Loss over Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance and Diagnostics - Confusion Matrix\n",
    "\n",
    "visualize the performance of the model by showing how often it correctly predicts each type of outcome\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Model A', 'Model B', 'Tie'], yticklabels=['Model A', 'Model B', 'Tie'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance and Diagnostics - Probability Calibration\n",
    "\n",
    "check if the model's confidence in its predictions aligns with the actual accuracy. If the model is confident in its predictions but the predictions are wrong, it indicates a need to calibrate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for i, (y_true, y_prob) in enumerate(zip(true_labels, predicted_probs)):\n",
    "    prob_true, prob_pred = calibration_curve(y_true == i, y_prob[:, i], n_bins=10)\n",
    "    plt.plot(prob_pred, prob_true, marker='o', label=f'Class {i}')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k:', label=\"Perfectly Calibrated\")\n",
    "plt.title(\"Calibration Curves\")\n",
    "plt.xlabel(\"Predicted Probability\")\n",
    "plt.ylabel(\"True Probability\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Kaggle Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_submission(model, test_loader):\n",
    "    model.eval()\n",
    "    submission = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            inputs, ids = batch\n",
    "            input_ids = inputs['input_ids'].squeeze(1).to(CFG.device)\n",
    "            attention_mask = inputs['attention_mask'].squeeze(1).to(CFG.device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            probabilities = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "            for i, id in enumerate(ids):\n",
    "                submission.append([id, probabilities[i, 0], probabilities[i, 1], probabilities[i, 2]])\n",
    "\n",
    "    submission_df = pd.DataFrame(submission, columns=['id', 'winner_model_a', 'winner_model_b', 'winner_tie'])\n",
    "    submission_df.to_csv('data/submission/submission.csv', index=False)\n",
    "\n",
    "generate_submission(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
